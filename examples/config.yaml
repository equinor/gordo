# Workflow generator configuration file compatible with `gordo`'s Kubernetes
# custom resource defintion.  You may also refer to the `config.yaml` file in
# the `gordo-test-project` repository
apiVersion: equinor.com/v1
kind: Gordo
metadata:
  name: test-project
spec:
  deploy-version: 0.32.0
  config:
    machines:

      - name: ct-23-0001 #1st machine
        dataset: |
          data_provider:
            type: RandomDataProvider
          tags: #list of tags for 1st machine
            - GRA-TAG 1
            - GRA-TAG 2
            - GRA-TAG 3
          train_start_date: 2016-11-07T09:11:30+01:00
          train_end_date: 2018-09-15T03:01:00+01:00

      - name: ct-23-0002 #2nd machine
        dataset: |
          data_provider:
            type: RandomDataProvider
          resolution: 2T
          tags: #list of tags for 2nd machine
            - GRA-TAG 1
            - GRA-TAG 2
            - GRA-TAG 3
          train_start_date: 2018-05-20T01:00:04+02:00
          train_end_date: 2019-05-10T15:05:50+02:00
        runtime: |
          builder: #one can override just a single resource request as well
            resources:
              requests:
                memory: 1000
          influx: # Dont push results for this single model to influx
            enable: False

      - name: ct-23-0003 #3rd machine
        dataset: |
          data_provider:
            type: RandomDataProvider
          tags: #list of tags for 3rd machine
            - GRA-TAG 1
            - GRA-TAG 2
            - GRA-TAG 3
          train_start_date: 2018-09-15T13:03:04+02:00
          train_end_date: 2019-03-11T11:05:10+02:00

    globals:
      runtime: |
        influx: # Change to False to disable influx. Default value: True
          enable: True
        builder: # The model builder can be customized per-machine, or set globally
          resources:
            requests:
              memory: 4000 #4 GB of memory
              cpu: 2000 #2 cores
            # since limits are not specified, the defaults are used (3GB memory and 32 cores CPU), but since the memory
            # limit (3GB) is less than request(4GB), which is not allowed, the memory limit will be lifted to match
            # the memory request (4GB).
        server:  # The model server can only be set globally
          resources:
            requests: # The resources we require reserved
              memory: 1700 # Request 1.7G of memory
              cpu: 2000 # Request two cores, while e.g. 250 would mean a quarter core
            limits: # The resources we are limited to using.
              memory: 2000 #Kill the server if it uses more than 2GB
              cpu: 2000 #Throttle the server to never using more than 2 cores
      model: |
        gordo.machine.model.anomaly.diff.DiffBasedAnomalyDetector:
          base_estimator:
            sklearn.pipeline.Pipeline:
              steps:
                - sklearn.preprocessing.MinMaxScaler
                - gordo.machine.model.models.KerasAutoEncoder:
                    kind: feedforward_hourglass
